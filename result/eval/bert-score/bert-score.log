
# fine-tuning model result

precision: 0.717812180519104, recall: 0.750409722328186, f1_score: 0.7337490916252136
precision: 0.7166804671287537, recall: 0.7717960476875305, f1_score: 0.7432178854942322
precision: 0.7721099257469177, recall: 0.7613877058029175, f1_score: 0.7667113542556763
precision: 0.736699104309082, recall: 0.7760515213012695, f1_score: 0.7558634877204895
Average precision: 0.7358254194259644
Average recall: 0.7649112492799759
Average f1_score: 0.7498854547739029

# base model result

precision: 0.6530874371528625, recall: 0.7230056524276733, f1_score: 0.6862702965736389
precision: 0.672155499458313, recall: 0.683645486831665, f1_score: 0.6778517961502075
precision: 0.6817560195922852, recall: 0.6792047619819641, f1_score: 0.680478036403656
precision: 0.6661725640296936, recall: 0.7472096681594849, f1_score: 0.7043679356575012
Average precision: 0.6682928800582886
Average recall: 0.7082663923501968
Average f1_score: 0.6872420161962509


# references

- https://www.nogawanogawa.com/entry/bertscore
- https://zenn.dev/ty_nlp/articles/d449dd2bc3aed4
